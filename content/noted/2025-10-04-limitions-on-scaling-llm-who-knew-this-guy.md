---
title: "limitions on scaling LLM, who knew.. this guy.."
author: "Commonplace Book Tools Bot"
date: "2025-10-04T23:43:15.002954+00:00"
generated: "2025-10-04T21:25:47-05:00"
tags:
  - ai
  - predictions
isBasedOn:
  type: "NewsArticle"
  headline: "What if A.I. Doesn’t Get Much Better Than This? | The New Yorker"
  url: "https://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this?utm_campaign=cm&utm_source=crm&utm_brand=tny&utm_mailing=TNY_SubPersRec_020520&utm_medium=email&bxid=5fc57aa0366dab66eb3a06a7&cndid=62977402&hasha=66583cd4412f4d9925aefd1046c8934a&hashb=21dafed87602e5d03c0d4fe850798aa14417ef7b&hashc=f17edc96ea2412e22607c11d9a406b3dad9f8c461f37bbdde057f93a1956bd30&utm_term=NYR_CYGNUS_PERSRECS"
  author: "Cal Newport (New Yorker)"
  datePublished: "August 12, 2025"
  publisher: "www.newyorker.com"
guid: "24b587e6-06e5-4a3c-bcc6-b3d03560b9ea"
---

> Some voices within the industry began to wonder if the A.I. scaling law was starting to falter. “The 2010s were the age of scaling, now we’re back in the age of wonder and discovery once again,” Ilya Sutskever, one of the company’s founders, told Reuters in November. “Everyone is looking for the next thing.” A contemporaneous TechCrunch article summarized the general mood: “Everyone now seems to be admitting you can’t just use more compute and more data while pretraining large language models and expect them to turn into some sort of all-knowing digital god.” But such observations were largely drowned out by the headline-generating rhetoric of other A.I. leaders. “A.I. is starting to get better than humans at almost all intellectual tasks,” Amodei recently told Anderson Cooper. In an interview with Axios, he predicted that half of entry-level white-collar jobs might be “wiped out” in the next one to five years. This summer, both Altman and Mark Zuckerberg, of Meta, claimed that their companies were close to developing superintelligence.

I think it was obvious from the beginning to those reading and understanding the technology behind LLMs that there was an upper boundary to LLM and models. I won't take away anyone's accomplishments. Every. Single. Model. has done better coding for me than the previous. But it STILL does silly things. And none of them are creating new technical patterns. Dependency injection and factory patterns are still created by engineers needing to solve a problem.

Or as Dr. Zaius says. even a primate is able to perform basic mimicry.

---

<sub>Quote Citation: <cite>Cal Newport (New Yorker), "What if A.I. Doesn’t Get Much Better Than This? | The New Yorker", August 12, 2025, <a href="https://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this?utm_campaign=cm&utm_source=crm&utm_brand=tny&utm_mailing=TNY_SubPersRec_020520&utm_medium=email&bxid=5fc57aa0366dab66eb3a06a7&cndid=62977402&hasha=66583cd4412f4d9925aefd1046c8934a&hashb=21dafed87602e5d03c0d4fe850798aa14417ef7b&hashc=f17edc96ea2412e22607c11d9a406b3dad9f8c461f37bbdde057f93a1956bd30&utm_term=NYR_CYGNUS_PERSRECS">https://www.newyorker.com/culture/open-questions/what-if-ai-doesnt-get-much-better-than-this?utm_campaign=cm&utm_source=crm&utm_brand=tny&utm_mailing=TNY_SubPersRec_020520&utm_medium=email&bxid=5fc57aa0366dab66eb3a06a7&cndid=62977402&hasha=66583cd4412f4d9925aefd1046c8934a&hashb=21dafed87602e5d03c0d4fe850798aa14417ef7b&hashc=f17edc96ea2412e22607c11d9a406b3dad9f8c461f37bbdde057f93a1956bd30&utm_term=NYR_CYGNUS_PERSRECS</a></cite></sub>