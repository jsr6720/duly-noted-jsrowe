---
title: "How AI coding assistants could be compromised via rules file | SC Media"
author: "Commonplace Book Tools Bot"
date: "2025-03-23T22:14:12-05:00"
draft: "False"
slug: "news_how-ai-coding-assistants-could-be-compromised-via-rules-file"
tags:
  - ai
  - security
source: "www.scworld.com"
original_url: "https://www.scworld.com/news/how-ai-coding-assistants-could-be-compromised-via-rules-file"
guid: "b685e7b4-078e-41a0-af68-9686fec58dd0"
---

> The Pillar researchers found that both GitHub Copilot and Cursor followed the instructions to add the external script when asked to generate an HTML page, and that the addition of this script was not mentioned in the assistantsâ€™ natural language responses.

Ah, not so fast. I told the AI 'No mistakes'.