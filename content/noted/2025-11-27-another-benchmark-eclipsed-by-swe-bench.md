---
title: another benchmark eclipsed by swe-bench
author: Commonplace Book Tools Bot
date: '2025-11-27T23:50:59.517853+00:00'
generated: '2025-11-29T12:47:08-05:00'
tags:
- ai
- predictions
isBasedOn:
  type: NewsArticle
  headline: 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive
    Programming?'
  url: https://arxiv.org/pdf/2506.11928
  author: arxiv
  datePublished: '2025-06-13'
  publisher: arxiv.org
guid: 946d67d5-6bce-4f9a-a348-e104870d33cc
---

> In this work, we introduce LiveCodeBench Pro, a rigorously curated and contamination-free bench-mark designed to evaluate the true algorithmic reasoning capabilities of LLMs in competitive pro-gramming. 

Right but have they seen what excel can do? Regardless, average cost per problem analysis I think is premature. And the distinction of 'reasoning' and non-reasoning AIs also feels like a lost cause. 

My prediction: Convergence in the future will be on price per token / problem solve rate.

---

<sub>Quote Citation: <cite>arxiv, "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", 2025-06-13, <a href="https://arxiv.org/pdf/2506.11928">https://arxiv.org/pdf/2506.11928</a></cite></sub>